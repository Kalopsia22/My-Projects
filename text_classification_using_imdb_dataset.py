# -*- coding: utf-8 -*-
"""Text Classification using IMDB Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDghkWIibFQ3TymOhjyPmu80iyCfeIZu
"""

pip install transformers datasets

from datasets import load_dataset

dataset = load_dataset('imdb')

train_dataset = dataset['train']
test_dataset = dataset['test']

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(example):
    return tokenizer(example['text'], padding='max_length', truncation=True)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

import torch

train_dataset = train_dataset.remove_columns(['text'])
train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

test_dataset = test_dataset.remove_columns(['text'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy='epoch',     # evaluation strategy to adopt during training
    num_train_epochs=3,              # number of training epochs
    per_device_train_batch_size=16,  # batch size for training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                      # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,               # training arguments, defined above
    train_dataset=train_dataset,      # training dataset
    eval_dataset=test_dataset         # evaluation dataset
)

trainer.train()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

preds = trainer.predict(test_dataset)
preds = preds.predictions.argmax(-1)

labels = test_dataset['label']
accuracy = accuracy_score(labels, preds)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")